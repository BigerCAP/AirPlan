<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>K8S 安装记录 - 真神了 - TiDB AirPlan - A biger solution</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Jeff" /><meta name="description" content="0x00 缘起 无它，就是学习学习，然后装逼用 0x01 准备 K8S 有一些要求限制，不按照处理是无法启动滴 以下内容随手复制的，非原创 关闭 swap 1 2 3 4 5 6 7 8 9 10 11 12 sed" /><meta name="keywords" content="TiDB, AirPlan, TiDB OPS, tidb-operator, 2pc, raft, databases, mysql, htap" />






<meta name="generator" content="Hugo 0.63.2 with theme even" />


<link rel="canonical" href="https://ap.tidb.cc/post/20200403-14-k8s-install/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<link href="/dist/even.04bd84cd.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="K8S 安装记录 - 真神了" />
<meta property="og:description" content="0x00 缘起 无它，就是学习学习，然后装逼用 0x01 准备 K8S 有一些要求限制，不按照处理是无法启动滴 以下内容随手复制的，非原创 关闭 swap 1 2 3 4 5 6 7 8 9 10 11 12 sed" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ap.tidb.cc/post/20200403-14-k8s-install/" />
<meta property="article:published_time" content="2020-04-03T00:00:00+08:00" />
<meta property="article:modified_time" content="2020-04-03T00:00:00+08:00" />
<meta itemprop="name" content="K8S 安装记录 - 真神了">
<meta itemprop="description" content="0x00 缘起 无它，就是学习学习，然后装逼用 0x01 准备 K8S 有一些要求限制，不按照处理是无法启动滴 以下内容随手复制的，非原创 关闭 swap 1 2 3 4 5 6 7 8 9 10 11 12 sed">
<meta itemprop="datePublished" content="2020-04-03T00:00:00&#43;08:00" />
<meta itemprop="dateModified" content="2020-04-03T00:00:00&#43;08:00" />
<meta itemprop="wordCount" content="6614">



<meta itemprop="keywords" content="Linux,testing,deploy,K8S,kubernetes," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="K8S 安装记录 - 真神了"/>
<meta name="twitter:description" content="0x00 缘起 无它，就是学习学习，然后装逼用 0x01 准备 K8S 有一些要求限制，不按照处理是无法启动滴 以下内容随手复制的，非原创 关闭 swap 1 2 3 4 5 6 7 8 9 10 11 12 sed"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">TiDB AirPlan</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/post/">
        <li class="mobile-menu-item">Blog</li>
      </a><a href="/look/">
        <li class="mobile-menu-item">Look</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">TiDB AirPlan</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/post/">Blog</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/look/">Look</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">K8S 安装记录 - 真神了</h1>

      <div class="post-meta">
        <span class="post-time"> 2020-04-03 </span>
        <div class="post-category">
            <a href="/categories/software/"> software </a>
            </div>
          <span class="more-meta"> 6614 words </span>
          <span class="more-meta"> 14 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#0x00-缘起">0x00 缘起</a></li>
    <li><a href="#0x01-准备">0x01 准备</a></li>
    <li><a href="#0x02-docker-ce">0x02 Docker-ce</a></li>
    <li><a href="#0x03-k8s">0x03 K8S</a>
      <ul>
        <li><a href="#init">Init</a></li>
        <li><a href="#flannel">Flannel</a></li>
        <li><a href="#testing">Testing</a></li>
      </ul>
    </li>
    <li><a href="#0x04-nodes">0x04 Nodes</a>
      <ul>
        <li><a href="#join">Join</a></li>
        <li><a href="#remove">Remove</a></li>
      </ul>
    </li>
    <li><a href="#0x05-faq">0x05 FAQ</a>
      <ul>
        <li><a href="#case-0">case 0</a></li>
        <li><a href="#case-1">case 1</a></li>
        <li><a href="#case-2">case 2</a></li>
        <li><a href="#case-3">case 3</a></li>
        <li><a href="#case-4">case 4</a></li>
        <li><a href="#case-5">case 5</a></li>
        <li><a href="#case-6">case 6</a></li>
        <li><a href="#case-7">case 7</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
  <div class="post-outdated">
    <div class="hint">
      <p>[NOTE] Updated <span class="timeago" datetime="2020-04-03T00:00:00" title="April 3, 2020">April 3, 2020</span>. This article may have outdated content or subject matter.</p>
    </div>
  </div>
    <div class="post-content">
      <h2 id="0x00-缘起">0x00 缘起</h2>
<blockquote>
<p>无它，就是学习学习，然后装逼用</p>
</blockquote>
<p><img src="/global/helpertools.jpg" alt="无它，唯手熟尔"></p>
<h2 id="0x01-准备">0x01 准备</h2>
<blockquote>
<p>K8S 有一些要求限制，不按照处理是无法启动滴<br>
以下内容随手复制的，非原创</p>
</blockquote>
<ul>
<li>
<p>关闭 swap</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sed -i 7,9s/0/1/g /usr/lib/sysctl.d/00-system.conf

modprobe br_netfilter

<span class="nb">echo</span> <span class="s1">&#39;vm.swappiness = 0&#39;</span> &gt;&gt; /usr/lib/sysctl.d/00-system.conf

swapoff -a

sysctl -p /usr/lib/sysctl.d/00-system.conf

<span class="nb">echo</span> “net.ipv4.ip_forward <span class="o">=</span> 1” &gt;&gt; /etc/sysctl.conf <span class="o">&amp;&amp;</span> sysctl -p
<span class="c1"># 开启 ipv4 forward</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>关闭防火墙</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="c1"># 关闭 selinux</span>
systemctl status firewalld
systemctl status iptables
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h2 id="0x02-docker-ce">0x02 Docker-ce</h2>
<blockquote>
<p>之前安装 docker 的时候都是 yum install docker，当时应该安装是 docker-io ，这是最早的版本。docker 估计是为了商业化，后来改造成了 docker-ce 和 docker-ee 两个版本
Docker 社区版（CE）：为了开发人员或小团队创建基于容器的应用, 与团队成员分享和自动化的开发管道。docker-ce 提供了简单的安装和快速的安装，以便可以立即开始开发。docker-ce 集成和优化，基础设施。（免费）
Docker 企业版（EE）：专为企业的发展和 IT 团队建立谁。docker-ee 为企业提供最安全的容器平台，以应用为中心的平台。（付费）</p>
</blockquote>
<ul>
<li>
<p>修改国内 aliyun yum 源</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">wget -O  /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
<span class="c1"># 获取 aliyun yum 源</span>

yum install docker-ce-18.09.6*
<span class="c1"># TiDB operator 需要指定 docker 版本</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>查看 yum 是否安装成功</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="o">[</span>root@tmp49h <span class="o">]</span><span class="c1"># rpm -qa | grep docker</span>
docker-ce-cli-19.03.8-3.el7.x86_64
docker-ce-18.09.6-3.el7.x86_64
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>选配 / 设置 docker-registry-mirrors 为国内地址</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">tee /etc/docker/daemon.json <span class="s">&lt;&lt;-&#39;EOF&#39;
</span><span class="s">{
</span><span class="s">&#34;registry-mirrors&#34;: [&#34;https://kzflpq4b.mirror.aliyuncs.com&#34;]
</span><span class="s">}
</span><span class="s">EOF</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>简单操作命令</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">systemctl <span class="nb">enable</span> docker
<span class="c1"># 添加开机启动</span>

systemctl start docker
<span class="c1"># 使用 systemd 启动 docker</span>

systemctl status docker
<span class="c1"># 查看 doker 运行状态</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>查看 docker 版本信息</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="o">[</span>root@tmp49h <span class="o">]</span><span class="c1"># docker version</span>
Client: Docker Engine - Community
Version:           19.03.8
API version:       1.39 <span class="o">(</span>downgraded from 1.40<span class="o">)</span>
Go version:        go1.12.17
Git commit:        afacb8b
Built:             Wed Mar <span class="m">11</span> 01:27:04 <span class="m">2020</span>
OS/Arch:           linux/amd64
Experimental:      <span class="nb">false</span>

Server: Docker Engine - Community
Engine:
  Version:          18.09.6
  API version:      1.39 <span class="o">(</span>minimum version 1.12<span class="o">)</span>
  Go version:       go1.10.8
  Git commit:       481bc77
  Built:            Sat May  <span class="m">4</span> 02:02:43 <span class="m">2019</span>
  OS/Arch:          linux/amd64
  Experimental:     <span class="nb">false</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>简单 docker 命令</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">docker images

docker pull xxxx:tag

docker ps  / docker ps -a

docker logs name

docker rm -f name

</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h2 id="0x03-k8s">0x03 K8S</h2>
<blockquote>
<p>安装简单，关键是如何规避科学上网</p>
</blockquote>
<ul>
<li>
<p>安装 kubectl</p>
<ul>
<li>默认读取 $HOME/.kube/config 文件来操作 K8S 集群</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.12.8/bin/linux/amd64/kubectl

chmod <span class="m">755</span> kubectl

mv kubectl /use/bin/
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>安装 helm</p>
<ul>
<li>注意：脚本中把 helm 默认放在了 <code>/usr/local/bin/</code>，如果脚本执行失败记得检查下</li>
<li>PS ：可以把脚本下载到本地 helm.sh ，执行 sh -x helm.sh ，可以看到 debug 记录</li>
<li>PPS：该脚本多次执行会报错，自身不带清理回收</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get <span class="p">|</span> bash
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>安装 kube 套件</p>
<ul>
<li>如果安装出现 gpg 无效，诡异的是执行下 yum makecache 就好了</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">tee /etc/yum.repos.d/kubernets.repo <span class="s">&lt;&lt; EOF
</span><span class="s">[Kubernetes]
</span><span class="s">name=kubernetes 
</span><span class="s">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
</span><span class="s">gpgcheck=1
</span><span class="s">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg,https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg,https://mirrors.aliyun.com/kubernetes/yum/doc/apt-key.gpg
</span><span class="s">EOF</span>
<span class="c1"># 添加 yum 源</span>

yum makecache
<span class="c1">#创建yum元数据</span>

yum install -y kubelet kubeadm kubectl
<span class="c1"># 可选直接安装 kubectl ，也可以手动安装</span>
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="init">Init</h3>
<blockquote>
<p>这里有多种方案，均需要主机可以自由联网下载数据，比如：</p>
<ul>
<li>Docker 代理</li>
<li>http / socket 代理</li>
<li>指定 kubeadm 源</li>
<li>iptable 转发 k8s.grc.io 域名到其他国内镜像源域名 / 搭建 DNS 做 cname 转发也可以</li>
<li>私自搭建 docker hub / ~~ 奇技淫巧~~ 可以用于内网机器搭建 k8s 环境</li>
</ul>
<p>我是懒人，选择指定 kubeadm 源。会用到一个 <code>kubeadm config print init-default</code> 的命令</p>
</blockquote>
<ul>
<li>
<p>执行 kubeadm init 前建议先用 <code>kubeadm config images pull</code> 测试下是否可以获取镜像</p>
<ul>
<li>以下是失败案例，提示链接不到 <code>https://k8s.gcr.io/v2/</code> 仓库</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="o">[</span>root@tmp49h <span class="o">]</span><span class="c1"># kubeadm config images pull</span>
W0402 10:19:46.190059    <span class="m">3816</span> configset.go:202<span class="o">]</span> WARNING: kubeadm cannot validate component configs <span class="k">for</span> API groups <span class="o">[</span>kubelet.config.k8s.io kubeproxy.config.k8s.io<span class="o">]</span>

failed to pull image <span class="s2">&#34;k8s.gcr.io/kube-apiserver:v1.18.0&#34;</span>: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled <span class="k">while</span> waiting <span class="k">for</span> connection <span class="o">(</span>Client.Timeout exceeded <span class="k">while</span> awaiting headers<span class="o">)</span>
, error: <span class="nb">exit</span> status <span class="m">1</span>
To see the stack trace of this error execute with --v<span class="o">=</span><span class="m">5</span> or higher
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>那么接来就要使用备用方案处理</p>
<ul>
<li>此处使用的修改数据源</li>
<li>具体详情见 <a href="#case-1">FAQ case-1</a></li>
<li>然后执行 <code>kubeadm init --config ./config</code> 命令</li>
</ul>
</li>
<li>
<p>安装记录</p>
<ul>
<li>失败的过程 <a href="#case-4">FAQ case-4</a></li>
<li>成功的过程 <a href="#case-5">FAQ case-5</a></li>
</ul>
</li>
<li>
<p>记得记录安装成功的输出内容，如下</p>
<ul>
<li>尤其是最底部的 kubeadm join 信息，该 token 时效 24 小时</li>
<li>通过 kubeadm token list 可以看到信息</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &#34;kubectl apply -f [podnetwork].yaml&#34; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.0.1.5:6443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:9a4b14630871dd9c8c6cd19787a305ef1188337375682085459d5d5bcd2d2b52
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="o">[</span>root@tmp49h ~<span class="o">]</span><span class="c1"># kubeadm token list</span>
TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
abcdef.0123456789abcdef   6h          2020-04-03T12:05:16Z   authentication,signing   &lt;none&gt;                                                     system:bootstrappers:kubeadm:default-node-token
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="flannel">Flannel</h3>
<ul>
<li>
<p>k8s 网络插件 flannel 安装，安装没遇见问题，</p>
<ul>
<li>该步骤需要下载镜像，所以网络上可能会耗时</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="o">[</span>root@tmp49h ~<span class="o">]</span><span class="c1"># kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span>
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds-amd64 created
daemonset.apps/kube-flannel-ds-arm64 created
daemonset.apps/kube-flannel-ds-arm created
daemonset.apps/kube-flannel-ds-ppc64le created
daemonset.apps/kube-flannel-ds-s390x created
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="testing">Testing</h3>
<blockquote>
<p>测试是否可用</p>
</blockquote>
<ul>
<li>
<p>执行几个查询命令</p>
<ul>
<li><code>kubectl get node</code></li>
<li><code>kubectl get namespace</code></li>
<li><code>kubectl get pods --namespace=kube-system -o wide</code></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="o">[</span>root@tmp49h ~<span class="o">]</span><span class="c1"># kubectl get node</span>
NAME     STATUS   ROLES    AGE     VERSION
tmp49h   Ready    master   7m21s   v1.18.0

<span class="o">[</span>root@tmp49h ~<span class="o">]</span><span class="c1">#  kubectl get namespace</span>
NAME              STATUS   AGE
default           Active   7m26s
kube-node-lease   Active   7m28s
kube-public       Active   7m28s
kube-system       Active   7m28s

<span class="o">[</span>root@tmp49h ~<span class="o">]</span><span class="c1"># kubectl get pods --namespace=kube-system -o wide</span>
NAME                             READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
coredns-6fbfdf9657-54gp9         1/1     Running   <span class="m">0</span>          7m29s   192.240.0.2   tmp49h   &lt;none&gt;           &lt;none&gt;
coredns-6fbfdf9657-hwpb2         1/1     Running   <span class="m">0</span>          7m29s   192.240.0.3   tmp49h   &lt;none&gt;           &lt;none&gt;
etcd-tmp49h                      1/1     Running   <span class="m">0</span>          7m43s   10.0.1.5      tmp49h   &lt;none&gt;           &lt;none&gt;
kube-apiserver-tmp49h            1/1     Running   <span class="m">0</span>          7m43s   10.0.1.5      tmp49h   &lt;none&gt;           &lt;none&gt;
kube-controller-manager-tmp49h   1/1     Running   <span class="m">0</span>          7m43s   10.0.1.5      tmp49h   &lt;none&gt;           &lt;none&gt;
kube-flannel-ds-amd64-9bs72      1/1     Running   <span class="m">0</span>          55s     10.0.1.5      tmp49h   &lt;none&gt;           &lt;none&gt;
kube-proxy-zdln9                 1/1     Running   <span class="m">0</span>          7m29s   10.0.1.5      tmp49h   &lt;none&gt;           &lt;none&gt;
kube-scheduler-tmp49h            1/1     Running   <span class="m">0</span>          7m43s   10.0.1.5      tmp49h   &lt;none&gt;           &lt;none&gt;
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h2 id="0x04-nodes">0x04 Nodes</h2>
<ul>
<li>
<p>准备环境 / 按照 <a href="#0x01-%e5%87%86%e5%a4%87">0x01</a> 做一遍</p>
<ul>
<li>重点检查下有木有 ip_vs_sh、ip_vs_wrr、ip_vs_rr、ip_vs 的这几个模块，木有的话尝试用 modprobe 加载</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="o">[</span>root@tmp49h ~<span class="o">]</span><span class="c1"># lsmod |grep ip_</span>
ip_vs_sh               <span class="m">12688</span>  <span class="m">0</span>
ip_vs_wrr              <span class="m">12697</span>  <span class="m">0</span>
ip_vs_rr               <span class="m">12600</span>  <span class="m">0</span>
ip_vs                 <span class="m">145497</span>  <span class="m">6</span> ip_vs_rr,ip_vs_sh,ip_vs_wrr
nf_conntrack          <span class="m">139224</span>  <span class="m">7</span> ip_vs,nf_nat,nf_nat_ipv4,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_netlink,nf_conntrack_ipv4
ip_tables              <span class="m">27126</span>  <span class="m">4</span> iptable_security,iptable_filter,iptable_mangle,iptable_nat
libcrc32c              <span class="m">12644</span>  <span class="m">4</span> xfs,ip_vs,nf_nat,nf_conntrack
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p><strong>按照 <a href="#0x02-docker-ce">0x02</a> 安装 docker-ce 服务</strong></p>
</li>
<li>
<p>接下来是获取必备镜像</p>
<ul>
<li><code>gcr.azk8s.cn/google_containers/kube-proxy:v1.18.0</code></li>
<li><code>quay.io/coreos/flannel:v0.12.0-amd64</code></li>
<li><code>gcr.azk8s.cn/google_containers/pause:3.2</code></li>
</ul>
</li>
<li>
<p>下载镜像方式与 k8s init 方式类似</p>
<ul>
<li><code>kubeadm config print init-defaults</code> 修改源，详情见<a href="#case-1">FAQ case-1</a></li>
<li><code>kubeadm --config ./config config images pull</code> 下载镜像</li>
<li><code>docker pull quay.io/coreos/flannel:v0.12.0-amd64</code> ，<strong>这个要单独下</strong></li>
</ul>
</li>
<li>
<p>检查 docker 镜像</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="o">[</span>root@atman-50 atman<span class="o">]</span><span class="c1"># docker images</span>
REPOSITORY                                               TAG                 IMAGE ID            CREATED             SIZE
gcr.azk8s.cn/google_containers/kube-proxy                v1.18.0             43940c34f24f        <span class="m">7</span> days ago          117MB
gcr.azk8s.cn/google_containers/kube-apiserver            v1.18.0             74060cea7f70        <span class="m">7</span> days ago          173MB
gcr.azk8s.cn/google_containers/kube-controller-manager   v1.18.0             d3e55153f52f        <span class="m">7</span> days ago          162MB
gcr.azk8s.cn/google_containers/kube-scheduler            v1.18.0             a31f78c7c8ce        <span class="m">7</span> days ago          95.3MB
quay.io/coreos/flannel                                   v0.12.0-amd64       4e9f801d2217        <span class="m">2</span> weeks ago         52.8MB
gcr.azk8s.cn/google_containers/pause                     3.2                 80d28bedfe5d        <span class="m">6</span> weeks ago         683kB
gcr.azk8s.cn/google_containers/coredns                   1.6.7               67da37a9a360        <span class="m">2</span> months ago        43.8MB
gcr.azk8s.cn/google_containers/etcd                      3.4.3-0             303ce5db0e90        <span class="m">5</span> months ago        288MB
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="join">Join</h3>
<ul>
<li>
<p>执行 kubeadm join 命令，token 过期后见 <a href="#case-6">FAQ case-6</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="o">[</span>root@atman-50 atman<span class="o">]</span><span class="c1"># kubeadm join 10.0.1.5:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:9a4b14630871dd9c8c6cd19787a305ef1188337375682085459d5d5bcd2d2b52</span>
W0402 12:43:22.333783    <span class="m">3043</span> join.go:346<span class="o">]</span> <span class="o">[</span>preflight<span class="o">]</span> WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
<span class="o">[</span>preflight<span class="o">]</span> Running pre-flight checks
  <span class="o">[</span>WARNING IsDockerSystemdCheck<span class="o">]</span>: detected <span class="s2">&#34;cgroupfs&#34;</span> as the Docker cgroup driver. The recommended driver is <span class="s2">&#34;systemd&#34;</span>. Please follow the guide at https://kubernetes.io/docs/setup/cri/
<span class="o">[</span>preflight<span class="o">]</span> Reading configuration from the cluster...
<span class="o">[</span>preflight<span class="o">]</span> FYI: You can look at this config file with <span class="s1">&#39;kubectl -n kube-system get cm kubeadm-config -oyaml&#39;</span>
<span class="o">[</span>kubelet-start<span class="o">]</span> Downloading configuration <span class="k">for</span> the kubelet from the <span class="s2">&#34;kubelet-config-1.18&#34;</span> ConfigMap in the kube-system namespace
<span class="o">[</span>kubelet-start<span class="o">]</span> Writing kubelet configuration to file <span class="s2">&#34;/var/lib/kubelet/config.yaml&#34;</span>
<span class="o">[</span>kubelet-start<span class="o">]</span> Writing kubelet environment file with flags to file <span class="s2">&#34;/var/lib/kubelet/kubeadm-flags.env&#34;</span>
<span class="o">[</span>kubelet-start<span class="o">]</span> Starting the kubelet
<span class="o">[</span>kubelet-start<span class="o">]</span> Waiting <span class="k">for</span> the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run <span class="s1">&#39;kubectl get nodes&#39;</span> on the control-plane to see this node join the cluster.
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="remove">Remove</h3>
<ul>
<li>
<p>删除 node</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl drain tmp50h --delete-local-data --force --ignore-daemonsets
<span class="c1"># 设置被目标节点为维护模式【atman-50 是节点名称】，通过 kubectl get node 获取</span>

kubectl delete node atman-50
<span class="c1"># 删除节点</span>
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h2 id="0x05-faq">0x05 FAQ</h2>
<blockquote>
<p>记录下测试期间遇见的大大小小的各种坑</p>
</blockquote>
<h3 id="case-0">case 0</h3>
<blockquote>
<p>机器配置太低！</p>
</blockquote>
<ul>
<li>
<p>心痛，安装 k8s 至少需要 2 个 cpu</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">W0402 10:03:26.595866    3844 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
[init] Using Kubernetes version: v1.18.0
[preflight] Running pre-flight checks
  [WARNING Service-Docker]: docker service is not enabled, please run &#39;systemctl enable docker.service&#39;
  [WARNING IsDockerSystemdCheck]: detected &#34;cgroupfs&#34; as the Docker cgroup driver. The recommended driver is &#34;systemd&#34;. Please follow the guide at https://kubernetes.io/docs/setup/cri/
error execution phase preflight: [preflight] Some fatal errors occurred:
  [ERROR NumCPU]: the number of available CPUs 1 is less than the required 2
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="case-1">case 1</h3>
<blockquote>
<p>kubeadm 是个神器<br>
命令行参数不能覆盖配置文件参数，由于没有提前阅读文档，后面有多个坑</p>
</blockquote>
<ul>
<li>
<p>kubeadm 配置文件预览</p>
<ul>
<li>具体可以看 k8s 官网 <a href="https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/">kubeadm-init</a>【中文内容】</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="o">[</span>root@tmp49h ~<span class="o">]</span><span class="c1"># kubeadm config print init-defaults</span>
W0403 05:19:28.449746   <span class="m">23654</span> configset.go:202<span class="o">]</span> WARNING: kubeadm cannot validate component configs <span class="k">for</span> API groups <span class="o">[</span>kubelet.config.k8s.io kubeproxy.config.k8s.io<span class="o">]</span>

apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef  // token 密钥，注意修改
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4  // 如果是多个 master 节点，填写 master LB IP，init 的时候需要这个 IP
  bindPort: <span class="m">6443</span>
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: tmp49h
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
  
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: <span class="o">{</span><span class="o">}</span>
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io  // kubeadm 使用的源，可以修改为 gcr.azk8s.cn/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.18.0
networking:
  dnsDomain: cluster.local
  podSubnet: 192.240.0.0/16    // POD 预分配网段地址不可以与 node 主机地址重复
  serviceSubnet: 192.96.0.0/12 // 不知道干啥的
scheduler: <span class="o">{</span><span class="o">}</span>
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>命令行参数与配置文件内容冲突，无法覆盖</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@tmp49h ~]# kubeadm --config .kube/config init --kubernetes-version=stable-1 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12
can not mix &#39;--config&#39; with arguments [kubernetes-version pod-network-cidr service-cidr]
To see the stack trace of this error execute with --v=5 or higher
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>advertiseAddress 参数配置错误</p>
<ul>
<li>配置文件中指定 api.advertiseAddress 为 vip 使客户端通过 vip 来访问，指定 etcd.endpoints 使用我们已经搭建好的 etcd 集群存储集群数据，指定 imageRepository 使用我们自己的镜像仓库</li>
<li><code>https://www.niuhp.com/k8s/init-master1.html</code></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">I0402 11:54:28.334273   <span class="m">45830</span> round_trippers.go:443<span class="o">]</span> GET https://1.2.3.4:6443/healthz?timeout<span class="o">=</span>10s  in <span class="m">10000</span> milliseconds
I0402 11:54:38.835316   <span class="m">45830</span> round_trippers.go:443<span class="o">]</span> GET https://1.2.3.4:6443/healthz?timeout<span class="o">=</span>10s  in <span class="m">10000</span> milliseconds
I0402 11:54:49.334834   <span class="m">45830</span> round_trippers.go:443<span class="o">]</span> GET https://1.2.3.4:6443/healthz?timeout<span class="o">=</span>10s  in <span class="m">10000</span> milliseconds
<span class="o">[</span>kubelet-check<span class="o">]</span> Initial timeout of 40s passed.
I0402 11:54:59.834973   <span class="m">45830</span> round_trippers.go:443<span class="o">]</span> GET https://1.2.3.4:6443/healthz?timeout<span class="o">=</span>10s  in <span class="m">10000</span> milliseconds
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="case-2">case 2</h3>
<ul>
<li>
<p>因为频繁修改 config 文件，且 advertiseAddress 修改后，kubeadm 并不会覆盖已有的残留信息( k8s 在 /etc/kubernetes 下面生成的文件，提示文件内容与最新的 config 不一致)</p>
<ul>
<li>建议使用 kubeadm reset 重制</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">a kubeconfig file <span class="s2">&#34;/etc/kubernetes/kubelet.conf&#34;</span> exists already but has got the wrong API Server URL
k8s.io/kubernetes/cmd/kubeadm/app/phases/kubeconfig.validateKubeConfig

I0402 11:56:34.170449   <span class="m">46888</span> loader.go:375<span class="o">]</span> Config loaded from file:  /etc/kubernetes/admin.conf
a kubeconfig file <span class="s2">&#34;/etc/kubernetes/admin.conf&#34;</span> exists already but has got the wrong API Server URL

I0402 12:00:15.020692   <span class="m">49830</span> loader.go:375<span class="o">]</span> Config loaded from file:  /etc/kubernetes/controller-manager.conf
failed to find CurrentContext in Contexts of the kubeconfig file /etc/kubernetes/controller-manager.conf
k8s.io/kubernetes/cmd/kubeadm/app/phases/kubeconfig.validateKubeConfig
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>执行 kubeadm reset 重制环境信息</p>
<ul>
<li>提示重制了所有信息</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="o">[</span>root@tmp49h atman<span class="o">]</span><span class="c1"># kubeadm reset</span>
<span class="o">[</span>reset<span class="o">]</span> WARNING: Changes made to this host by <span class="s1">&#39;kubeadm init&#39;</span> or <span class="s1">&#39;kubeadm join&#39;</span> will be reverted.
<span class="o">[</span>reset<span class="o">]</span> Are you sure you want to proceed? <span class="o">[</span>y/N<span class="o">]</span>: y
<span class="o">[</span>preflight<span class="o">]</span> Running pre-flight checks
W0402 10:42:29.820727    <span class="m">7017</span> removeetcdmember.go:79<span class="o">]</span> <span class="o">[</span>reset<span class="o">]</span> No kubeadm config, using etcd pod spec to get data directory
<span class="o">[</span>reset<span class="o">]</span> No etcd config found. Assuming external etcd
<span class="o">[</span>reset<span class="o">]</span> Please, manually reset etcd to prevent further issues
<span class="o">[</span>reset<span class="o">]</span> Stopping the kubelet service
<span class="o">[</span>reset<span class="o">]</span> Unmounting mounted directories in <span class="s2">&#34;/var/lib/kubelet&#34;</span>
W0402 10:42:29.836562    <span class="m">7017</span> cleanupnode.go:99<span class="o">]</span> <span class="o">[</span>reset<span class="o">]</span> Failed to evaluate the <span class="s2">&#34;/var/lib/kubelet&#34;</span> directory. Skipping its unmount and cleanup: lstat /var/lib/kubelet: no such file or directory
<span class="o">[</span>reset<span class="o">]</span> Deleting contents of config directories: <span class="o">[</span>/etc/kubernetes/manifests /etc/kubernetes/pki<span class="o">]</span>
<span class="o">[</span>reset<span class="o">]</span> Deleting files: <span class="o">[</span>/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf<span class="o">]</span>
<span class="o">[</span>reset<span class="o">]</span> Deleting contents of stateful directories: <span class="o">[</span>/var/lib/dockershim /var/run/kubernetes /var/lib/cni<span class="o">]</span>

The reset process does not clean CNI configuration. To <span class="k">do</span> so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must <span class="k">do</span> so manually by using the <span class="s2">&#34;iptables&#34;</span> command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear <span class="o">(</span>or similar<span class="o">)</span>
to reset your system<span class="err">&#39;</span>s IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the <span class="nv">$HOME</span>/.kube/config file.
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="case-3">case 3</h3>
<ul>
<li>
<p>安装成功后，在 master 节点直接执行了 kubeadm join，提示信息被占用</p>
<ul>
<li>执行了 kubectl get node 才理解 master 本身就算一个节点，并且已经默认加入到了集群</li>
<li>所以加开了一台虚拟机测试 kubeadm join</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="o">[</span>root@tmp49h ~<span class="o">]</span><span class="c1"># kubeadm join 10.0.1.5:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:9a4b14630871dd9c8c6cd19787a305ef1188337375682085459d5d5bcd2d2b52</span>
W0402 12:14:46.722266   <span class="m">54428</span> join.go:346<span class="o">]</span> <span class="o">[</span>preflight<span class="o">]</span> WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
<span class="o">[</span>preflight<span class="o">]</span> Running pre-flight checks
  <span class="o">[</span>WARNING IsDockerSystemdCheck<span class="o">]</span>: detected <span class="s2">&#34;cgroupfs&#34;</span> as the Docker cgroup driver. The recommended driver is <span class="s2">&#34;systemd&#34;</span>. Please follow the guide at https://kubernetes.io/docs/setup/cri/
error execution phase preflight: <span class="o">[</span>preflight<span class="o">]</span> Some fatal errors occurred:
  <span class="o">[</span>ERROR DirAvailable--etc-kubernetes-manifests<span class="o">]</span>: /etc/kubernetes/manifests is not empty
  <span class="o">[</span>ERROR FileAvailable--etc-kubernetes-kubelet.conf<span class="o">]</span>: /etc/kubernetes/kubelet.conf already exists
  <span class="o">[</span>ERROR Port-10250<span class="o">]</span>: Port <span class="m">10250</span> is in use
  <span class="o">[</span>ERROR FileAvailable--etc-kubernetes-pki-ca.crt<span class="o">]</span>: /etc/kubernetes/pki/ca.crt already exists
<span class="o">[</span>preflight<span class="o">]</span> If you know what you are doing, you can make a check non-fatal with <span class="sb">`</span>--ignore-preflight-errors<span class="o">=</span>...<span class="sb">`</span>
To see the stack trace of this error execute with --v<span class="o">=</span><span class="m">5</span> or higher
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="case-4">case 4</h3>
<ul>
<li>
<p>贴个安装失败的日志</p>
<ul>
<li>该过程使用默认日志等级，有了错误啥也看不出来</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@tmp49h ~]# kubeadm --config .kube/config init 
W0402 11:30:17.970803   38671 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
[init] Using Kubernetes version: v1.18.0
[preflight] Running pre-flight checks
  [WARNING IsDockerSystemdCheck]: detected &#34;cgroupfs&#34; as the Docker cgroup driver. The recommended driver is &#34;systemd&#34;. Please follow the guide at https://kubernetes.io/docs/setup/cri/
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;
[kubelet-start] Writing kubelet environment file with flags to file &#34;/var/lib/kubelet/kubeadm-flags.env&#34;
[kubelet-start] Writing kubelet configuration to file &#34;/var/lib/kubelet/config.yaml&#34;
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder &#34;/etc/kubernetes/pki&#34;
[certs] Generating &#34;ca&#34; certificate and key
[certs] Generating &#34;apiserver&#34; certificate and key
[certs] apiserver serving cert is signed for DNS names [tmp49h kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 1.2.3.4]
[certs] Generating &#34;apiserver-kubelet-client&#34; certificate and key
[certs] Generating &#34;front-proxy-ca&#34; certificate and key
[certs] Generating &#34;front-proxy-client&#34; certificate and key
[certs] Generating &#34;etcd/ca&#34; certificate and key
[certs] Generating &#34;etcd/server&#34; certificate and key
[certs] etcd/server serving cert is signed for DNS names [tmp49h localhost] and IPs [1.2.3.4 127.0.0.1 ::1]
[certs] Generating &#34;etcd/peer&#34; certificate and key
[certs] etcd/peer serving cert is signed for DNS names [tmp49h localhost] and IPs [1.2.3.4 127.0.0.1 ::1]
[certs] Generating &#34;etcd/healthcheck-client&#34; certificate and key
[certs] Generating &#34;apiserver-etcd-client&#34; certificate and key
[certs] Generating &#34;sa&#34; key and public key
[kubeconfig] Using kubeconfig folder &#34;/etc/kubernetes&#34;
[kubeconfig] Writing &#34;admin.conf&#34; kubeconfig file
[kubeconfig] Writing &#34;kubelet.conf&#34; kubeconfig file
[kubeconfig] Writing &#34;controller-manager.conf&#34; kubeconfig file
[kubeconfig] Writing &#34;scheduler.conf&#34; kubeconfig file
[control-plane] Using manifest folder &#34;/etc/kubernetes/manifests&#34;
[control-plane] Creating static Pod manifest for &#34;kube-apiserver&#34;
[control-plane] Creating static Pod manifest for &#34;kube-controller-manager&#34;
W0402 11:30:26.193263   38671 manifests.go:225] the default kube-apiserver authorization-mode is &#34;Node,RBAC&#34;; using &#34;Node,RBAC&#34;
[control-plane] Creating static Pod manifest for &#34;kube-scheduler&#34;
W0402 11:30:26.195753   38671 manifests.go:225] the default kube-apiserver authorization-mode is &#34;Node,RBAC&#34;; using &#34;Node,RBAC&#34;
[etcd] Creating static Pod manifest for local etcd in &#34;/etc/kubernetes/manifests&#34;
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &#34;/etc/kubernetes/manifests&#34;. This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

  Unfortunately, an error has occurred:
    timed out waiting for the condition

  This error is likely caused by:
    - The kubelet is not running
    - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

  If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
    - &#39;systemctl status kubelet&#39;
    - &#39;journalctl -xeu kubelet&#39;

  Additionally, a control plane component may have crashed or exited when started by the container runtime.
  To troubleshoot, list all containers using your preferred container runtimes CLI.

  Here is one example how you may list all Kubernetes containers running in docker:
    - &#39;docker ps -a | grep kube | grep -v pause&#39;
    Once you have found the failing container, you can inspect its logs with:
    - &#39;docker logs CONTAINERID&#39;

error execution phase wait-control-plane: couldn&#39;t initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="case-5">case 5</h3>
<ul>
<li>
<p>贴个安装成功的过程</p>
<ul>
<li>该过程使用了 -v=6 日志等级输出</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">[root@tmp49h ~]# kubeadm --config .kube/config init --ignore-preflight-errors=all --v=6
I0402 12:04:52.044950   52108 initconfiguration.go:200] loading configuration from &#34;.kube/config&#34;
I0402 12:04:52.048675   52108 interface.go:400] Looking for default routes with IPv4 addresses
I0402 12:04:52.048691   52108 interface.go:405] Default route transits interface &#34;eth0&#34;
I0402 12:04:52.049188   52108 interface.go:208] Interface eth0 is up
I0402 12:04:52.049282   52108 interface.go:256] Interface &#34;eth0&#34; has 2 addresses :[10.0.1.5/24 fe80::217:faff:fe00:a2b3/64].
I0402 12:04:52.049310   52108 interface.go:223] Checking addr  10.0.1.5/24.
I0402 12:04:52.049323   52108 interface.go:230] IP found 10.0.1.5
I0402 12:04:52.049337   52108 interface.go:262] Found valid IPv4 address 10.0.1.5 for interface &#34;eth0&#34;.
I0402 12:04:52.049348   52108 interface.go:411] Found active IP 10.0.1.5 
W0402 12:04:52.049539   52108 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
[init] Using Kubernetes version: v1.18.0
[preflight] Running pre-flight checks
I0402 12:04:52.049892   52108 checks.go:577] validating Kubernetes and kubeadm version
I0402 12:04:52.049924   52108 checks.go:166] validating if the firewall is enabled and active
I0402 12:04:52.076294   52108 checks.go:201] validating availability of port 6443
I0402 12:04:52.078115   52108 checks.go:201] validating availability of port 10259
I0402 12:04:52.078177   52108 checks.go:201] validating availability of port 10257
I0402 12:04:52.078232   52108 checks.go:286] validating the existence of file /etc/kubernetes/manifests/kube-apiserver.yaml
I0402 12:04:52.078252   52108 checks.go:286] validating the existence of file /etc/kubernetes/manifests/kube-controller-manager.yaml
I0402 12:04:52.078265   52108 checks.go:286] validating the existence of file /etc/kubernetes/manifests/kube-scheduler.yaml
I0402 12:04:52.078277   52108 checks.go:286] validating the existence of file /etc/kubernetes/manifests/etcd.yaml
I0402 12:04:52.078298   52108 checks.go:432] validating if the connectivity type is via proxy or direct
I0402 12:04:52.078353   52108 checks.go:471] validating http connectivity to first IP address in the CIDR
I0402 12:04:52.078381   52108 checks.go:471] validating http connectivity to first IP address in the CIDR
I0402 12:04:52.078402   52108 checks.go:102] validating the container runtime
I0402 12:04:52.224441   52108 checks.go:128] validating if the service is enabled and active
  [WARNING IsDockerSystemdCheck]: detected &#34;cgroupfs&#34; as the Docker cgroup driver. The recommended driver is &#34;systemd&#34;. Please follow the guide at https://kubernetes.io/docs/setup/cri/
I0402 12:04:52.388976   52108 checks.go:335] validating the contents of file /proc/sys/net/bridge/bridge-nf-call-iptables
I0402 12:04:52.389091   52108 checks.go:335] validating the contents of file /proc/sys/net/ipv4/ip_forward
I0402 12:04:52.389159   52108 checks.go:649] validating whether swap is enabled or not
I0402 12:04:52.389480   52108 checks.go:376] validating the presence of executable conntrack
I0402 12:04:52.389920   52108 checks.go:376] validating the presence of executable ip
I0402 12:04:52.390250   52108 checks.go:376] validating the presence of executable iptables
I0402 12:04:52.390276   52108 checks.go:376] validating the presence of executable mount
I0402 12:04:52.390532   52108 checks.go:376] validating the presence of executable nsenter
I0402 12:04:52.390558   52108 checks.go:376] validating the presence of executable ebtables
I0402 12:04:52.390574   52108 checks.go:376] validating the presence of executable ethtool
I0402 12:04:52.390596   52108 checks.go:376] validating the presence of executable socat
I0402 12:04:52.390614   52108 checks.go:376] validating the presence of executable tc
I0402 12:04:52.390629   52108 checks.go:376] validating the presence of executable touch
I0402 12:04:52.390648   52108 checks.go:520] running all checks
I0402 12:04:52.544118   52108 checks.go:406] checking whether the given node name is reachable using net.LookupHost
I0402 12:04:52.544592   52108 checks.go:618] validating kubelet version
I0402 12:04:52.649444   52108 checks.go:128] validating if the service is enabled and active
I0402 12:04:52.661720   52108 checks.go:201] validating availability of port 10250
I0402 12:04:52.661983   52108 checks.go:201] validating availability of port 2379
I0402 12:04:52.662091   52108 checks.go:201] validating availability of port 2380
I0402 12:04:52.662153   52108 checks.go:249] validating the existence and emptiness of directory /var/lib/etcd
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;
I0402 12:04:52.732912   52108 checks.go:838] image exists: gcr.azk8s.cn/google_containers/kube-apiserver:v1.18.0
I0402 12:04:52.803307   52108 checks.go:838] image exists: gcr.azk8s.cn/google_containers/kube-controller-manager:v1.18.0
I0402 12:04:52.874891   52108 checks.go:838] image exists: gcr.azk8s.cn/google_containers/kube-scheduler:v1.18.0
I0402 12:04:52.953082   52108 checks.go:838] image exists: gcr.azk8s.cn/google_containers/kube-proxy:v1.18.0
I0402 12:04:53.028397   52108 checks.go:838] image exists: gcr.azk8s.cn/google_containers/pause:3.2
I0402 12:04:53.105915   52108 checks.go:838] image exists: gcr.azk8s.cn/google_containers/etcd:3.4.3-0
I0402 12:04:53.179742   52108 checks.go:838] image exists: gcr.azk8s.cn/google_containers/coredns:1.6.7
I0402 12:04:53.179794   52108 kubelet.go:64] Stopping the kubelet
[kubelet-start] Writing kubelet environment file with flags to file &#34;/var/lib/kubelet/kubeadm-flags.env&#34;
[kubelet-start] Writing kubelet configuration to file &#34;/var/lib/kubelet/config.yaml&#34;
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder &#34;/etc/kubernetes/pki&#34;
I0402 12:04:53.517439   52108 certs.go:103] creating a new certificate authority for ca
[certs] Generating &#34;ca&#34; certificate and key
[certs] Generating &#34;apiserver&#34; certificate and key
[certs] apiserver serving cert is signed for DNS names [tmp49h kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [192.96.0.1 10.0.1.5]
[certs] Generating &#34;apiserver-kubelet-client&#34; certificate and key
I0402 12:04:54.676701   52108 certs.go:103] creating a new certificate authority for front-proxy-ca
[certs] Generating &#34;front-proxy-ca&#34; certificate and key
[certs] Generating &#34;front-proxy-client&#34; certificate and key
I0402 12:04:56.537529   52108 certs.go:103] creating a new certificate authority for etcd-ca
[certs] Generating &#34;etcd/ca&#34; certificate and key
[certs] Generating &#34;etcd/server&#34; certificate and key
[certs] etcd/server serving cert is signed for DNS names [tmp49h localhost] and IPs [10.0.1.5 127.0.0.1 ::1]
[certs] Generating &#34;etcd/peer&#34; certificate and key
[certs] etcd/peer serving cert is signed for DNS names [tmp49h localhost] and IPs [10.0.1.5 127.0.0.1 ::1]
[certs] Generating &#34;etcd/healthcheck-client&#34; certificate and key
[certs] Generating &#34;apiserver-etcd-client&#34; certificate and key
I0402 12:04:58.028007   52108 certs.go:69] creating new public/private key files for signing service account users
[certs] Generating &#34;sa&#34; key and public key
[kubeconfig] Using kubeconfig folder &#34;/etc/kubernetes&#34;
I0402 12:04:58.417808   52108 kubeconfig.go:79] creating kubeconfig file for admin.conf
[kubeconfig] Writing &#34;admin.conf&#34; kubeconfig file
I0402 12:04:59.079377   52108 kubeconfig.go:79] creating kubeconfig file for kubelet.conf
[kubeconfig] Writing &#34;kubelet.conf&#34; kubeconfig file
I0402 12:04:59.330894   52108 kubeconfig.go:79] creating kubeconfig file for controller-manager.conf
[kubeconfig] Writing &#34;controller-manager.conf&#34; kubeconfig file
I0402 12:04:59.559569   52108 kubeconfig.go:79] creating kubeconfig file for scheduler.conf
[kubeconfig] Writing &#34;scheduler.conf&#34; kubeconfig file
[control-plane] Using manifest folder &#34;/etc/kubernetes/manifests&#34;
[control-plane] Creating static Pod manifest for &#34;kube-apiserver&#34;
I0402 12:04:59.782909   52108 manifests.go:91] [control-plane] getting StaticPodSpecs
I0402 12:04:59.783878   52108 manifests.go:104] [control-plane] adding volume &#34;ca-certs&#34; for component &#34;kube-apiserver&#34;
I0402 12:04:59.783895   52108 manifests.go:104] [control-plane] adding volume &#34;etc-pki&#34; for component &#34;kube-apiserver&#34;
I0402 12:04:59.783905   52108 manifests.go:104] [control-plane] adding volume &#34;k8s-certs&#34; for component &#34;kube-apiserver&#34;
I0402 12:04:59.795764   52108 manifests.go:121] [control-plane] wrote static Pod manifest for component &#34;kube-apiserver&#34; to &#34;/etc/kubernetes/manifests/kube-apiserver.yaml&#34;
[control-plane] Creating static Pod manifest for &#34;kube-controller-manager&#34;
I0402 12:04:59.795793   52108 manifests.go:91] [control-plane] getting StaticPodSpecs
W0402 12:04:59.795959   52108 manifests.go:225] the default kube-apiserver authorization-mode is &#34;Node,RBAC&#34;; using &#34;Node,RBAC&#34;
I0402 12:04:59.796292   52108 manifests.go:104] [control-plane] adding volume &#34;ca-certs&#34; for component &#34;kube-controller-manager&#34;
I0402 12:04:59.796308   52108 manifests.go:104] [control-plane] adding volume &#34;etc-pki&#34; for component &#34;kube-controller-manager&#34;
I0402 12:04:59.796318   52108 manifests.go:104] [control-plane] adding volume &#34;flexvolume-dir&#34; for component &#34;kube-controller-manager&#34;
I0402 12:04:59.796327   52108 manifests.go:104] [control-plane] adding volume &#34;k8s-certs&#34; for component &#34;kube-controller-manager&#34;
I0402 12:04:59.796336   52108 manifests.go:104] [control-plane] adding volume &#34;kubeconfig&#34; for component &#34;kube-controller-manager&#34;
I0402 12:04:59.797418   52108 manifests.go:121] [control-plane] wrote static Pod manifest for component &#34;kube-controller-manager&#34; to &#34;/etc/kubernetes/manifests/kube-controller-manager.yaml&#34;
[control-plane] Creating static Pod manifest for &#34;kube-scheduler&#34;
I0402 12:04:59.797444   52108 manifests.go:91] [control-plane] getting StaticPodSpecs
W0402 12:04:59.797514   52108 manifests.go:225] the default kube-apiserver authorization-mode is &#34;Node,RBAC&#34;; using &#34;Node,RBAC&#34;
I0402 12:04:59.797782   52108 manifests.go:104] [control-plane] adding volume &#34;kubeconfig&#34; for component &#34;kube-scheduler&#34;
I0402 12:04:59.798954   52108 manifests.go:121] [control-plane] wrote static Pod manifest for component &#34;kube-scheduler&#34; to &#34;/etc/kubernetes/manifests/kube-scheduler.yaml&#34;
[etcd] Creating static Pod manifest for local etcd in &#34;/etc/kubernetes/manifests&#34;
I0402 12:04:59.800928   52108 local.go:72] [etcd] wrote Static Pod manifest for a local etcd member to &#34;/etc/kubernetes/manifests/etcd.yaml&#34;
I0402 12:04:59.800958   52108 waitcontrolplane.go:87] [wait-control-plane] Waiting for the API server to be healthy
I0402 12:04:59.802317   52108 loader.go:375] Config loaded from file:  /etc/kubernetes/admin.conf
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &#34;/etc/kubernetes/manifests&#34;. This can take up to 4m0s
I0402 12:04:59.804411   52108 round_trippers.go:443] GET https://10.0.1.5:6443/healthz?timeout=10s  in 0 milliseconds
I0402 12:05:00.305351   52108 round_trippers.go:443] GET https://10.0.1.5:6443/healthz?timeout=10s  in 0 milliseconds
I0402 12:05:00.805367   52108 round_trippers.go:443] GET https://10.0.1.5:6443/healthz?timeout=10s  in 0 milliseconds
I0402 12:05:01.305315   52108 round_trippers.go:443] GET https://10.0.1.5:6443/healthz?timeout=10s  in 0 milliseconds
I0402 12:05:06.305168   52108 round_trippers.go:443] GET https://10.0.1.5:6443/healthz?timeout=10s  in 0 milliseconds
I0402 12:05:06.809393   52108 round_trippers.go:443] GET https://10.0.1.5:6443/healthz?timeout=10s  in 2 milliseconds
I0402 12:05:13.635622   52108 round_trippers.go:443] GET https://10.0.1.5:6443/healthz?timeout=10s 500 Internal Server Error in 6330 milliseconds
I0402 12:05:13.807469   52108 round_trippers.go:443] GET https://10.0.1.5:6443/healthz?timeout=10s 500 Internal Server Error in 2 milliseconds
I0402 12:05:14.307830   52108 round_trippers.go:443] GET https://10.0.1.5:6443/healthz?timeout=10s 500 Internal Server Error in 2 milliseconds
I0402 12:05:14.809268   52108 round_trippers.go:443] GET https://10.0.1.5:6443/healthz?timeout=10s 500 Internal Server Error in 4 milliseconds
I0402 12:05:15.307555   52108 round_trippers.go:443] GET https://10.0.1.5:6443/healthz?timeout=10s 500 Internal Server Error in 2 milliseconds
I0402 12:05:15.809277   52108 round_trippers.go:443] GET https://10.0.1.5:6443/healthz?timeout=10s 200 OK in 4 milliseconds
[apiclient] All control plane components are healthy after 16.005791 seconds
I0402 12:05:15.809404   52108 uploadconfig.go:108] [upload-config] Uploading the kubeadm ClusterConfiguration to a ConfigMap
[upload-config] Storing the configuration used in ConfigMap &#34;kubeadm-config&#34; in the &#34;kube-system&#34; Namespace
I0402 12:05:15.815779   52108 round_trippers.go:443] POST https://10.0.1.5:6443/api/v1/namespaces/kube-system/configmaps?timeout=10s 201 Created in 4 milliseconds
I0402 12:05:15.824007   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles?timeout=10s 201 Created in 7 milliseconds
I0402 12:05:15.829199   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings?timeout=10s 201 Created in 4 milliseconds
I0402 12:05:15.829943   52108 uploadconfig.go:122] [upload-config] Uploading the kubelet component config to a ConfigMap
[kubelet] Creating a ConfigMap &#34;kubelet-config-1.18&#34; in namespace kube-system with the configuration for the kubelets in the cluster
I0402 12:05:15.834057   52108 round_trippers.go:443] POST https://10.0.1.5:6443/api/v1/namespaces/kube-system/configmaps?timeout=10s 201 Created in 2 milliseconds
I0402 12:05:15.841508   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles?timeout=10s 201 Created in 7 milliseconds
I0402 12:05:15.845856   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings?timeout=10s 201 Created in 4 milliseconds
I0402 12:05:15.845997   52108 uploadconfig.go:127] [upload-config] Preserving the CRISocket information for the control-plane node
I0402 12:05:15.846066   52108 patchnode.go:30] [patchnode] Uploading the CRI Socket information &#34;/var/run/dockershim.sock&#34; to the Node API object &#34;tmp49h&#34; as an annotation
I0402 12:05:16.352672   52108 round_trippers.go:443] GET https://10.0.1.5:6443/api/v1/nodes/tmp49h?timeout=10s 200 OK in 6 milliseconds
I0402 12:05:16.364925   52108 round_trippers.go:443] PATCH https://10.0.1.5:6443/api/v1/nodes/tmp49h?timeout=10s 200 OK in 5 milliseconds
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node tmp49h as control-plane by adding the label &#34;node-role.kubernetes.io/master=&#39;&#39;&#34;
[mark-control-plane] Marking the node tmp49h as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
I0402 12:05:16.868754   52108 round_trippers.go:443] GET https://10.0.1.5:6443/api/v1/nodes/tmp49h?timeout=10s 200 OK in 2 milliseconds
I0402 12:05:16.877566   52108 round_trippers.go:443] PATCH https://10.0.1.5:6443/api/v1/nodes/tmp49h?timeout=10s 200 OK in 7 milliseconds
[bootstrap-token] Using token: abcdef.0123456789abcdef
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0402 12:05:16.884804   52108 round_trippers.go:443] GET https://10.0.1.5:6443/api/v1/namespaces/kube-system/secrets/bootstrap-token-abcdef?timeout=10s 404 Not Found in 6 milliseconds
I0402 12:05:16.893841   52108 round_trippers.go:443] POST https://10.0.1.5:6443/api/v1/namespaces/kube-system/secrets?timeout=10s 201 Created in 8 milliseconds
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0402 12:05:16.908160   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/rbac.authorization.k8s.io/v1/clusterroles?timeout=10s 201 Created in 13 milliseconds
I0402 12:05:16.914868   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?timeout=10s 201 Created in 5 milliseconds
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0402 12:05:16.922119   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?timeout=10s 201 Created in 6 milliseconds
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0402 12:05:16.926440   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?timeout=10s 201 Created in 4 milliseconds
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0402 12:05:16.935959   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?timeout=10s 201 Created in 9 milliseconds
[bootstrap-token] Creating the &#34;cluster-info&#34; ConfigMap in the &#34;kube-public&#34; namespace
I0402 12:05:16.936118   52108 clusterinfo.go:45] [bootstrap-token] loading admin kubeconfig
I0402 12:05:16.937438   52108 loader.go:375] Config loaded from file:  /etc/kubernetes/admin.conf
I0402 12:05:16.937464   52108 clusterinfo.go:53] [bootstrap-token] copying the cluster from admin.conf to the bootstrap kubeconfig
I0402 12:05:16.937878   52108 clusterinfo.go:65] [bootstrap-token] creating/updating ConfigMap in kube-public namespace
I0402 12:05:16.944203   52108 round_trippers.go:443] POST https://10.0.1.5:6443/api/v1/namespaces/kube-public/configmaps?timeout=10s 201 Created in 6 milliseconds
I0402 12:05:16.944386   52108 clusterinfo.go:79] creating the RBAC rules for exposing the cluster-info ConfigMap in the kube-public namespace
I0402 12:05:16.952131   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-public/roles?timeout=10s 201 Created in 7 milliseconds
I0402 12:05:16.960717   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-public/rolebindings?timeout=10s 201 Created in 8 milliseconds
I0402 12:05:16.962014   52108 kubeletfinalize.go:88] [kubelet-finalize] Assuming that kubelet client certificate rotation is enabled: found &#34;/var/lib/kubelet/pki/kubelet-client-current.pem&#34;
[kubelet-finalize] Updating &#34;/etc/kubernetes/kubelet.conf&#34; to point to a rotatable kubelet client certificate and key
I0402 12:05:16.962805   52108 loader.go:375] Config loaded from file:  /etc/kubernetes/kubelet.conf
I0402 12:05:16.964191   52108 kubeletfinalize.go:132] [kubelet-finalize] Restarting the kubelet to enable client certificate rotation
I0402 12:05:17.144392   52108 round_trippers.go:443] GET https://10.0.1.5:6443/apis/apps/v1/namespaces/kube-system/deployments?labelSelector=k8s-app%3Dkube-dns 200 OK in 14 milliseconds
I0402 12:05:17.160533   52108 round_trippers.go:443] GET https://10.0.1.5:6443/api/v1/namespaces/kube-system/configmaps/kube-dns?timeout=10s 404 Not Found in 7 milliseconds
I0402 12:05:17.167540   52108 round_trippers.go:443] GET https://10.0.1.5:6443/api/v1/namespaces/kube-system/configmaps/coredns?timeout=10s 404 Not Found in 5 milliseconds
I0402 12:05:17.177800   52108 round_trippers.go:443] POST https://10.0.1.5:6443/api/v1/namespaces/kube-system/configmaps?timeout=10s 201 Created in 9 milliseconds
I0402 12:05:17.183843   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/rbac.authorization.k8s.io/v1/clusterroles?timeout=10s 201 Created in 5 milliseconds
I0402 12:05:17.192713   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?timeout=10s 201 Created in 8 milliseconds
I0402 12:05:17.204923   52108 round_trippers.go:443] POST https://10.0.1.5:6443/api/v1/namespaces/kube-system/serviceaccounts?timeout=10s 201 Created in 7 milliseconds
I0402 12:05:17.241253   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/apps/v1/namespaces/kube-system/deployments?timeout=10s 201 Created in 25 milliseconds
I0402 12:05:17.259595   52108 round_trippers.go:443] POST https://10.0.1.5:6443/api/v1/namespaces/kube-system/services?timeout=10s 201 Created in 15 milliseconds
[addons] Applied essential addon: CoreDNS
I0402 12:05:17.269186   52108 round_trippers.go:443] POST https://10.0.1.5:6443/api/v1/namespaces/kube-system/serviceaccounts?timeout=10s 201 Created in 9 milliseconds
I0402 12:05:17.279073   52108 round_trippers.go:443] POST https://10.0.1.5:6443/api/v1/namespaces/kube-system/configmaps?timeout=10s 201 Created in 7 milliseconds
I0402 12:05:17.303603   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/apps/v1/namespaces/kube-system/daemonsets?timeout=10s 201 Created in 15 milliseconds
I0402 12:05:17.312854   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings?timeout=10s 201 Created in 8 milliseconds
I0402 12:05:17.318643   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles?timeout=10s 201 Created in 5 milliseconds
I0402 12:05:17.355049   52108 round_trippers.go:443] POST https://10.0.1.5:6443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings?timeout=10s 201 Created in 36 milliseconds
[addons] Applied essential addon: kube-proxy
I0402 12:05:17.357075   52108 loader.go:375] Config loaded from file:  /etc/kubernetes/admin.conf
I0402 12:05:17.357930   52108 loader.go:375] Config loaded from file:  /etc/kubernetes/admin.conf

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &#34;kubectl apply -f [podnetwork].yaml&#34; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.0.1.5:6443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:9a4b14630871dd9c8c6cd19787a305ef1188337375682085459d5d5bcd2d2b52
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="case-6">case 6</h3>
<ul>
<li>kubeadm join 使用的 token 默认有效期 24 小时
<ul>
<li>通过 <code>kubeadm token list</code> 查看</li>
<li>过期后通过 <code>kubeadm token create</code> 创建</li>
</ul>
</li>
<li><code>--discovery-token-ca-cert-hash</code> 丢失处理方式，然后用新的token和ca-hash加入集群
<ul>
<li><code>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&amp;gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'</code></li>
</ul>
</li>
</ul>
<h3 id="case-7">case 7</h3>
<ul>
<li>
<p>这错误还没查到是啥意思</p>
<ul>
<li>以为是根据主机名无法解析，本地添加了 /etc/hosts 也没用</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">4月 02 11:46:47 tmp49h kubelet[42431]: E0402 11:46:47.388209   42431 kubelet.go:2267] node &#34;tmp49h&#34; not found
4月 02 11:46:47 tmp49h kubelet[42431]: I0402 11:46:47.455578   42431 kubelet_node_status.go:294] Setting node annotation to enable volume controller attach/detach
4月 02 11:46:47 tmp49h kubelet[42431]: I0402 11:46:47.456129   42431 kubelet_node_status.go:294] Setting node annotation to enable volume controller attach/detach
4月 02 11:46:47 tmp49h kubelet[42431]: I0402 11:46:47.458294   42431 topology_manager.go:219] [topologymanager] RemoveContainer - Container ID: 3633a94c2ab917bf6e58ed4e851ecaffbeed5bf13f9dc8a843100cd0da5d40f4
4月 02 11:46:47 tmp49h kubelet[42431]: I0402 11:46:47.459307   42431 topology_manager.go:219] [topologymanager] RemoveContainer - Container ID: 9bc11dafd96de2fad4223d92b8a5871f785841f84a03b31899294a944ebc4b03
4月 02 11:46:47 tmp49h kubelet[42431]: E0402 11:46:47.459989   42431 pod_workers.go:191] Error syncing pod 49c6953d8c6d010ee4fd5a17c89d4f21 (&#34;etcd-tmp49h_kube-system(49c6953d8c6d010ee4fd5a17c89d4f21)&#34;), skipping: failed to &#34;StartContainer&#34; for &#34;etcd&#34; with CrashLoopBackOff: &#34;back-off 40s restarting failed container=etcd pod=etcd-tmp49h_kube-system(49c6953d8c6d010ee4fd5a17c89d4f21)&#34;
4月 02 11:50:43 tmp49h kubelet[43440]: E0402 11:50:43.508686   43440 pod_workers.go:191] Error syncing pod 69f26225068c8c6c0e2cfe490854d0b9 (&#34;kube-apiserver-tmp49h_kube-system(69f26225068c8c6c0e2cfe490854d0b9)&#34;), skipping: failed to &#34;StartContainer&#34; for &#34;kube-apiserver&#34; with CrashLoopBackOff: &#34;back-off 20s restarting failed container=kube-apiserver pod=kube-apiserver-tmp49h_kube-system(69f26225068c8c6c0e2cfe490854d0b9)&#34;
4月 02 11:50:43 tmp49h kubelet[43440]: E0402 11:50:43.573473   43440 kubelet.go:2267] node &#34;tmp49h&#34; not found
4月 02 11:50:44 tmp49h kubelet[43440]: I0402 11:50:44.875613   43440 kubelet_node_status.go:294] Setting node annotation to enable volume controller attach/detach
4月 02 11:50:44 tmp49h kubelet[43440]: I0402 11:50:44.879380   43440 topology_manager.go:219] [topologymanager] RemoveContainer - Container ID: 0562b4ea5f837a8e89ad447860be51c652a0665389ce1980359f4c5bf12c890f
4月 02 11:50:44 tmp49h kubelet[43440]: E0402 11:50:44.879901   43440 pod_workers.go:191] Error syncing pod 49c6953d8c6d010ee4fd5a17c89d4f21 (&#34;etcd-tmp49h_kube-system(49c6953d8c6d010ee4fd5a17c89d4f21)&#34;), skipping: failed to &#34;StartContainer&#34; for &#34;etcd&#34; with CrashLoopBackOff: &#34;back-off 40s restarting failed container=etcd pod=etcd-tmp49h_kube-system(49c6953d8c6d010ee4fd5a17c89d4f21)&#34;
4月 02 11:50:45 tmp49h kubelet[43440]: E0402 11:50:45.330608   43440 controller.go:136] failed to ensure node lease exists, will retry in 3.2s, error: Get https://1.2.3.4:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/tmp49h?timeout=10s: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
4月 02 11:50:47 tmp49h kubelet[43440]: W0402 11:50:47.133078   43440 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d

4月 02 11:50:47 tmp49h kubelet[43440]: E0402 11:50:47.568483   43440 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
</code></pre></td></tr></table>
</div>
</div></li>
</ul>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">Jeff</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2020-04-03
        
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a> ｜ 本文不带任何官方色彩，最终解释权归本站所有</span>
  </p>
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/linux/">Linux</a>
          <a href="/tags/testing/">testing</a>
          <a href="/tags/deploy/">deploy</a>
          <a href="/tags/k8s/">K8S</a>
          <a href="/tags/kubernetes/">kubernetes</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/20200408-15-k8s-pv/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">K8S 安装记录 - PV &amp; PVC</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/20200323-13-teamview/">
            <span class="next-text nav-default">使用软件时强制升级的设计</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="https://github.com/BigerCAP/TiDB-AirPlan/issues" class="iconfont icon-github" title="github"></a>
  <a href="https://ap.tidb.cc/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
      <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2017 - 
    2020
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">TiDB - AirPlan</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/timeago.js@3.0.2/dist/timeago.min.js" integrity="sha256-jwCP0NAdCBloaIWTWHmW4i3snUNMHUNO+jr9rYd2iOI=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/timeago.js@3.0.2/dist/timeago.locales.min.js" integrity="sha256-ZwofwC1Lf/faQCzN7nZtfijVV6hSwxjQMwXL4gn9qU8=" crossorigin="anonymous"></script>
  <script><!-- NOTE: timeago.js uses the language code format like "zh_CN" (underscore and case sensitive) -->
    var languageCode = "en".replace(/-/g, '_').replace(/_(.*)/, function ($0, $1) {return $0.replace($1, $1.toUpperCase());});
    timeago().render(document.querySelectorAll('.timeago'), languageCode);
    timeago.cancel();  
  </script>
<script type="text/javascript" src="/dist/even.26188efa.min.js"></script>








</body>
</html>
